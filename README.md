# Data Mastery in 90 Days

**Goal**: Build technical depth in data engineering, data science, and GenAI through hands-on projects while creating a compelling portfolio demonstrating practical expertise.

**Time Commitment**: 15-20 hours/week (3-4 hours on weekdays, 6-8 hours on weekends)

## First 30 Days

** Goal: Learn Data Engineering & Data Science Fundamentals

### Week 1-2: Core Infrastructure & Tools

**Learning Objectives:**

- Master Python for data engineering
- Understand cloud platforms (AWS/Azure/GCP)
- Learn containerization and orchestration basics

**Hands-on Activities:**

- Set up development environment (Python, Docker, cloud account)
- Build a simple ETL pipeline using Python and pandas
- Deploy a basic data pipeline to cloud (Lambda/Cloud Functions)

**Portfolio Project 1**: **Real-time Weather Data Pipeline**

- Extract data from weather APIs
- Transform and validate data
- Load into cloud database
- Create simple dashboard
- Document architecture and decisions

### Week 3-4: Advanced Data Engineering

**Learning Objectives:**

- Modern data stack (dbt, Airflow, Snowflake/BigQuery)
- Data modeling and warehousing concepts
- Data quality and monitoring

**Hands-on Activities:**

- Set up Airflow locally
- Learn dbt for data transformations
- Implement data quality checks

**Portfolio Project 2**: **E-commerce Analytics Pipeline**

- Simulate transactional data
- Build dimensional model with dbt
- Orchestrate with Airflow
- Implement data quality monitoring
- Create automated testing

## Next 30 Days

Goal: Data Analytics & MLOps

## Last 90 Days

Goal: Generative AI Apps
